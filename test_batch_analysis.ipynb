{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEM Particle Analysis - Test Notebook\n",
    "\n",
    "This notebook tests the refactored `sem_particle_analysis` package on a folder of SEM images.\n",
    "\n",
    "## What this notebook does:\n",
    "1. Loads the refactored Python package\n",
    "2. Processes SEM images from `/Users/sanjaypradeep/Downloads/Trial Images`\n",
    "3. Automatically detects scale bars\n",
    "4. Segments particles using SAM\n",
    "5. Analyzes particle sizes\n",
    "6. Saves results to CSV\n",
    "\n",
    "## Installation\n",
    "If you haven't installed the package yet:\n",
    "```bash\n",
    "cd sem_particle_analysis\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the refactored package\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the package to path (if not installed)\n",
    "package_path = os.path.join(os.getcwd(), 'sem_particle_analysis')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.insert(0, package_path)\n",
    "\n",
    "from sem_particle_analysis import (\n",
    "    SAMModel,\n",
    "    ScaleDetector,\n",
    "    ParticleSegmenter,\n",
    "    ParticleAnalyzer,\n",
    "    ResultsManager\n",
    ")\n",
    "from sem_particle_analysis.utils import (\n",
    "    load_image,\n",
    "    find_images_in_folder,\n",
    "    visualize_masks,\n",
    "    visualize_comparison,\n",
    "    plot_size_distribution,\n",
    "    print_summary\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ Packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAM_CHECKPOINT = \"/Users/sanjaypradeep/segment-anything/models/sam_vit_h_4b8939.pth\"\n",
    "MODEL_TYPE = \"vit_h\"  # Options: \"vit_h\" (best), \"vit_l\", \"vit_b\" (fastest)\n",
    "IMAGE_FOLDER = \"/Users/sanjaypradeep/Downloads/Trial Images\"\n",
    "OUTPUT_CSV = \"batch_analysis_results.csv\"\n",
    "\n",
    "# Analysis parameters\n",
    "MIN_PARTICLE_AREA = 50  # Minimum particle area in pixels\n",
    "CROP_PERCENT = 7.0  # Percentage to crop from bottom (for scale bar removal)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  SAM Model: {MODEL_TYPE}\")\n",
    "print(f\"  Image folder: {IMAGE_FOLDER}\")\n",
    "print(f\"  Output CSV: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SAM Model\n",
    "\n",
    "This loads the Segment Anything Model once and reuses it for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAM model\n",
    "print(\"Loading SAM model...\")\n",
    "sam_model = SAMModel(SAM_CHECKPOINT, model_type=MODEL_TYPE)\n",
    "print(\"✓ SAM model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Images in Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all images in the folder\n",
    "image_paths = find_images_in_folder(IMAGE_FOLDER)\n",
    "print(f\"Found {len(image_paths)} images in '{IMAGE_FOLDER}'\")\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    print(\"ERROR: No images found in the specified folder!\")\n",
    "else:\n",
    "    print(\"\\nFirst 5 images:\")\n",
    "    for i, path in enumerate(image_paths[:5], 1):\n",
    "        print(f\"  {i}. {os.path.basename(path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "scale_detector = ScaleDetector(use_gpu=False)\n",
    "segmenter = ParticleSegmenter(sam_model)\n",
    "results_manager = ResultsManager(OUTPUT_CSV)\n",
    "\n",
    "print(\"✓ All components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Single Image (Example)\n",
    "\n",
    "Let's process the first image as an example to see the full workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process first image as example\n",
    "if len(image_paths) > 0:\n",
    "    example_image_path = image_paths[0]\n",
    "    print(f\"Processing example image: {os.path.basename(example_image_path)}\")\n",
    "    \n",
    "    # 1. Load image\n",
    "    image = load_image(example_image_path)\n",
    "    print(f\"  Image size: {image.shape[1]} x {image.shape[0]} pixels\")\n",
    "    \n",
    "    # Display original image\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Original Image: {os.path.basename(example_image_path)}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Detect scale bar\n",
    "print(\"\\nDetecting scale bar...\")\n",
    "try:\n",
    "    scale_info = scale_detector.detect_scale_bar(\n",
    "        image,\n",
    "        region_width=0.5,\n",
    "        region_height=0.06,\n",
    "        vertical_offset=0.0,\n",
    "        threshold=250\n",
    "    )\n",
    "    \n",
    "    print(f\"  Scale bar detected:\")\n",
    "    print(f\"    Physical length: {scale_info['scale_nm']:.1f} nm\")\n",
    "    print(f\"    Pixel length: {scale_info['pixel_length']} pixels\")\n",
    "    print(f\"    Conversion: {scale_info['conversion']:.3f} nm/pixel\")\n",
    "    print(f\"    OCR text: '{scale_info['ocr_text']}'\")\n",
    "    \n",
    "    conversion_factor = scale_info['conversion']\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  Warning: Scale bar detection failed: {e}\")\n",
    "    print(\"  Using manual scale entry...\")\n",
    "    conversion_factor = scale_detector.set_manual_scale(scale_nm=100.0, pixel_length=50)\n",
    "    print(f\"  Manual conversion set to: {conversion_factor:.3f} nm/pixel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Crop scale bar from image\n",
    "cropped_image = scale_detector.crop_scale_bar(image, crop_percent=CROP_PERCENT)\n",
    "print(f\"\\nCropped image size: {cropped_image.shape[1]} x {cropped_image.shape[0]} pixels\")\n",
    "\n",
    "# Display cropped image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cropped_image)\n",
    "plt.title(\"Cropped Image (Scale Bar Removed)\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Segment particles with SAM\n",
    "print(\"\\nSegmenting particles with SAM...\")\n",
    "masks, scores = segmenter.segment_image(cropped_image, multimask_output=True)\n",
    "\n",
    "# Visualize mask candidates\n",
    "fig = visualize_masks(cropped_image, masks, scores, figsize=(18, 6))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGenerated {len(masks)} mask candidates\")\n",
    "print(f\"Confidence scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Select best mask\n",
    "selected_mask = segmenter.select_mask()  # Auto-selects highest score\n",
    "binary_mask = segmenter.get_binary_mask(invert=True)  # Particles = 1, background = 0\n",
    "\n",
    "print(f\"Selected mask {segmenter.selected_mask_index} with score {scores[segmenter.selected_mask_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyze particles\n",
    "print(\"\\nAnalyzing particles...\")\n",
    "analyzer = ParticleAnalyzer(conversion_factor=conversion_factor)\n",
    "num_particles, regions = analyzer.analyze_mask(\n",
    "    binary_mask,\n",
    "    min_area=MIN_PARTICLE_AREA,\n",
    "    min_size=30,\n",
    "    remove_border=True,\n",
    "    border_buffer=4\n",
    ")\n",
    "\n",
    "print(f\"\\nDetected {num_particles} particles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Get measurements\n",
    "measurements = analyzer.get_measurements(in_nm=True)\n",
    "\n",
    "# Print summary\n",
    "print_summary(measurements, title=\"Example Image Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Visualize results\n",
    "fig = visualize_comparison(cropped_image, analyzer.labeled_mask, analyzer.regions, figsize=(16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Plot size distributions\n",
    "fig = plot_size_distribution(measurements, bins=20, figsize=(12, 5))\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Process All Images\n",
    "\n",
    "Now let's process all images in the folder automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing function\n",
    "def process_image_auto(image_path, sam_model, scale_detector, segmenter, \n",
    "                       crop_percent=7.0, min_area=50):\n",
    "    \"\"\"\n",
    "    Process a single image automatically.\n",
    "    Returns measurements dict or None if processing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(image_path)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Load image\n",
    "        image = load_image(image_path)\n",
    "        \n",
    "        # Detect scale\n",
    "        try:\n",
    "            scale_info = scale_detector.detect_scale_bar(image)\n",
    "            conversion = scale_info['conversion']\n",
    "            print(f\"  Scale: {scale_info['scale_nm']:.1f} nm = {scale_info['pixel_length']} px\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Scale detection failed ({e}). Using manual scale.\")\n",
    "            conversion = scale_detector.set_manual_scale(scale_nm=100.0, pixel_length=50)\n",
    "        \n",
    "        # Crop image\n",
    "        cropped_image = scale_detector.crop_scale_bar(image, crop_percent=crop_percent)\n",
    "        \n",
    "        # Segment with SAM\n",
    "        masks, scores = segmenter.segment_image(cropped_image, multimask_output=True)\n",
    "        selected_mask = segmenter.select_mask()\n",
    "        binary_mask = segmenter.get_binary_mask(invert=True)\n",
    "        \n",
    "        # Analyze particles\n",
    "        analyzer = ParticleAnalyzer(conversion_factor=conversion)\n",
    "        num_particles, regions = analyzer.analyze_mask(\n",
    "            binary_mask,\n",
    "            min_area=min_area,\n",
    "            min_size=30,\n",
    "            remove_border=True,\n",
    "            border_buffer=4\n",
    "        )\n",
    "        \n",
    "        measurements = analyzer.get_measurements(in_nm=True)\n",
    "        print(f\"  Detected {num_particles} particles\")\n",
    "        \n",
    "        return filename, measurements\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Failed to process {filename}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"Batch processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all images\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BATCH PROCESSING {len(image_paths)} IMAGES\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "\n",
    "for i, image_path in enumerate(image_paths, 1):\n",
    "    print(f\"\\n[{i}/{len(image_paths)}]\", end=\" \")\n",
    "    \n",
    "    filename, measurements = process_image_auto(\n",
    "        image_path,\n",
    "        sam_model,\n",
    "        scale_detector,\n",
    "        segmenter,\n",
    "        crop_percent=CROP_PERCENT,\n",
    "        min_area=MIN_PARTICLE_AREA\n",
    "    )\n",
    "    \n",
    "    if measurements is not None:\n",
    "        # Save results\n",
    "        results_manager.add_result(filename, measurements)\n",
    "        successful += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"BATCH PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully processed: {successful}/{len(image_paths)}\")\n",
    "print(f\"Failed: {failed}/{len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of all results\n",
    "results_manager.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results dataframe\n",
    "results_df = results_manager.get_results()\n",
    "print(\"\\nResults DataFrame:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are automatically saved to the CSV file\n",
    "print(f\"Results saved to: {OUTPUT_CSV}\")\n",
    "print(f\"\\nYou can also export to a different file:\")\n",
    "\n",
    "# Example: Export to a custom filename\n",
    "# results_manager.export_results(\"my_custom_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Complete!\n",
    "\n",
    "The refactored package has successfully analyzed all SEM images.\n",
    "\n",
    "### Next Steps:\n",
    "1. Review the results in `batch_analysis_results.csv`\n",
    "2. Use the stored measurements for further statistical analysis\n",
    "3. Adjust parameters (min_area, crop_percent) if needed and rerun\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "- ✓ Automated scale bar detection\n",
    "- ✓ SAM-based particle segmentation  \n",
    "- ✓ Particle size measurements (area, diameter)\n",
    "- ✓ Batch processing of multiple images\n",
    "- ✓ Results storage and management\n",
    "- ✓ Visualization tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
